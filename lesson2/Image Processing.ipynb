{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   `lesson02`:  Image Processing I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the Python processing you have done before now has probably consisted of plain-text and numerical data.  Visual and signal data are often accessed as images.  If you want to build a computer process around images, then you need a standardized way of storing and cleaning data, identifying features, and applying transformations.\n",
    "\n",
    "**Objectives:**\n",
    "-   Distinguish image formats and data structures, including color and greyscale representations.\n",
    "-   Manipulate image\n",
    "-   Process images through a standardized pipeline for image analysis.\n",
    "-   Identify image features such as bones and tumors.\n",
    "\n",
    "\n",
    "This lesson is divided into two parts:\n",
    "\n",
    "1.  **(Class)**  We will cover the structure and format of image data and common features and statistics.\n",
    "2.  **(Team)**  You will fill out a basic image processing workflow using MRI brain scan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Some library boilerplate.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data,io,filters,exposure\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A picture, in the real world, is a two-dimensional representation of something.  That _something_ can be three-dimensional or itself flat.\n",
    "\n",
    "<img src=\"https://whitehotmagazine.com//UserFiles/image/2012/Buckminster%20Fuller/Fuller_LaminarGeodesic.jpg\" width=\"75%;\" alt=\"Buckminster Fuller's _Laminar Geodesic Dome_\"/>\n",
    "\n",
    "Since computer screens are flat, we frequently interact with data as pictures rather than something more volumentric.\n",
    "\n",
    "<img src=\"https://i.kinja-img.com/gawker-media/image/upload/s--K2mqoGa6--/c_scale,f_auto,fl_progressive,q_80,w_800/aflxtfyesukzvfd5abnl.png\" width=\"75%;\" alt=\"Princess Leia as a space-occupying hologram.\"/>\n",
    "\n",
    "A computer screen is a grid of square pixels.  Each pixel is capable of displaying itself in one of about 16 million colors.\n",
    "\n",
    "One can imagine storing a picture on a computer in a number of ways:  the actual location and value of each pixel (bitmaps); the path taken by a pen to redraw the image (vector graphics); or a decompression algorithm from numbers to a graphic (JPEG/PNG).  Image-processing libraries take care of that transformation for us and present an image as a collection of values in a grid corresponding to the pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, let's load a simple black-and-white image.  We will use the [_scikit-image_](https://scikit-image.org/) package to handle the file.\n",
    "\n",
    "<img src=\"./img/buckminster-fuller-geodesic-tensegrity-sphere_bw.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bw_path = './img/buckminster-fuller-geodesic-tensegrity-sphere_bw.png'\n",
    "image_bw = io.imread( image_bw_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the output of this image.  Look at its data type, its size and shape, and the values in it.  (How can you use NumPy to find these out?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?  What is the range of values?\n",
    "\n",
    "Black-and-white images are straightforward in that only two values need to be stored, typically `0` and `1` or `0` and `255`.  (More on why `255` below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a grayscale image now, something with more variation.\n",
    "\n",
    "<img src=\"./img/buckminster-fuller-geodesic-tensegrity-sphere_gray.png\">\n",
    "\n",
    "It looks very similar, but in fact has gray elements instead of just black ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gray_path = './img/buckminster-fuller-geodesic-tensegrity-sphere_gray.png'\n",
    "# load the image as image_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX REMOVEME\n",
    "image_gray = io.imread( image_gray_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, what is the range of values, the shape, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move in, it's worth considering the tools that we need to successfully analyze an image.  Thus far, you can either show an image as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or you can display it as an image again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow( image_bw )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow( image_gray )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the human eye fails to pick out features, however, you risk missing key elements of the image.  To offset this risk, we can use image statistics to obtain a more fully-faceted idea of what the picture contains.\n",
    "\n",
    "The first of these is the _histogram_.  Histograms, as you'll recall, count up the number of times something occurs.  For instance, for a class, the grade distribution of As, Bs, etc., is a histogram.  For an image, a histogram can describe the color distribution of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the histogram of the black-and-white image.\n",
    "hist_bw,hist_centers_bw = exposure.histogram( image_bw )\n",
    "\n",
    "# Get the histogram of the grayscale image.\n",
    "hist_gray,hist_centers_gray = exposure.histogram( image_gray )\n",
    "\n",
    "fig,ax = plt.subplots( ncols=2,figsize=( 10,8 ) )\n",
    "\n",
    "ax[ 0 ].plot( hist_centers_bw,hist_bw,lw=1 )\n",
    "ax[ 0 ].set_title( 'B&W' )\n",
    "ax[ 1 ].plot( hist_centers_gray,hist_gray,lw=1 )\n",
    "ax[ 1 ].set_title( 'Grayscale' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's subtle, but you can see that the image on the left has sharp peaks at black (`0`) and white (`255`), while the image on the right has soften peaks and some fuzziness along the line, indicating the presence of a few gray pixels at intermediate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other images would, of course, have different histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_hist_path = './img/ansel-adams-moon-and-half-dome.jpg'\n",
    "image_hist = io.imread( image_hist_path,as_gray=True )\n",
    "\n",
    "hist_,hist_centers_ = exposure.histogram( image_hist )\n",
    "\n",
    "fig,ax = plt.subplots( ncols=2,figsize=( 10,4 ) )\n",
    "\n",
    "ax[ 0 ].imshow( image_hist,cmap='Greys_r' )\n",
    "ax[ 0 ].set_title( 'Source Image' )\n",
    "ax[ 1 ].plot( hist_centers_,hist_,lw=1 )\n",
    "ax[ 1 ].set_title( 'Histogram' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If black-and-white and grayscale images are stored as values from `0` to `255`, how do computers deal with color?\n",
    "\n",
    "Computer screens use _additive color_, or color based on mixing red, green, and blue to produce other colors.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/AdditiveColor.svg/145px-AdditiveColor.svg.png)\n",
    "\n",
    "Each screen pixel is addressed by location and set with a particular value.  This value has [32 bits](https://en.wikipedia.org/wiki/Color_depth#True_color_(24-bit)):  one byte each to describe the amount of red, green, and blue light, and the _alpha_ channel, or transparency of the pixel.  (For a gray pixel, the color values are all set equal and the alpha is set to `255`.)\n",
    "\n",
    "![](./img/pixel_color.png)\n",
    "\n",
    "Every color that can be shown on a typical computer display is represented:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/16777216colors.png\" width=\"75%;\" alt=\"A square showing 16 million colors.\"/>\n",
    "\n",
    "(Of course, the intensity can vary from black to white as well.)\n",
    "\n",
    "Since there are three colors (ignoring alpha), what shape would you expect a loaded image to have as an array?  Let's check your intuition.\n",
    "\n",
    "![](./img/mandelbrot-set.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_color_path = './img/mandelbrot-set.png'\n",
    "# load the image as image_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX REMOVEME\n",
    "image_color = io.imread( image_color_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVEME\n",
    "image_color.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image, formerly just a grid, now has a third dimension, which we can think of as three layers.  The top layer corresponds to the redness of the image, the middle to the greenness, the bottom to the blueness.\n",
    "\n",
    "![](./img/image_layers.png)\n",
    "\n",
    "We now have more information about the image than we had before.  For instance, some feature we are interested in may only show up in one color channel, or in all three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, for instance, a satellite image.  If you are tracking snowfall or cloud cover, you know that those features are \"white\" and close to equal intensity in all color channels.  Something that is intense in red and green but not blue would appear yellow and may be a desert.\n",
    "\n",
    "![](./img/satellite-caucasus.jpg)\n",
    "\n",
    "By applying rules such as blue only for water and green only for vegetation, you can automate surveys and satellite analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our Mandelbrot set image.\n",
    "\n",
    "![](./img/mandelbrot-set.png)\n",
    "\n",
    "If we plot each color layer and its histogram separately, we can see how the features of the overall image interrelate.\n",
    "\n",
    "We do this for red together.  You should also do this for green and blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_r = image_color[ :,:,0 ].copy()\n",
    "hist_r,hist_centers_r = exposure.histogram( image_r )\n",
    "\n",
    "fig,ax = plt.subplots( ncols=2,figsize=( 10,4 ) )\n",
    "\n",
    "ax[ 0 ].imshow( image_r,cmap='binary_r' )\n",
    "ax[ 0 ].set_title( 'Source Image, Red Channel' )\n",
    "ax[ 1 ].plot( hist_centers_r,hist_r,lw=3 )\n",
    "ax[ 1 ].set_title( 'Histogram' )\n",
    "ax[ 1 ].set_ylim( ( 0,256**2 ) )\n",
    "ax[ 1 ].set_xlim( ( 0,255 ) )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVEME\n",
    "image_g = image_color[ :,:,1 ].copy()\n",
    "hist_g,hist_centers_g = exposure.histogram( image_g )\n",
    "\n",
    "fig,ax = plt.subplots( ncols=2,figsize=( 10,4 ) )\n",
    "\n",
    "ax[ 0 ].imshow( image_g,cmap='binary_r' )\n",
    "ax[ 0 ].set_title( 'Source Image, Green Channel' )\n",
    "ax[ 1 ].plot( hist_centers_g,hist_g,lw=3 )\n",
    "ax[ 1 ].set_title( 'Histogram' )\n",
    "ax[ 1 ].set_ylim( ( 0,256**2 ) )\n",
    "ax[ 1 ].set_xlim( ( 0,255 ) )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVEME\n",
    "image_b = image_color[ :,:,2 ].copy()\n",
    "hist_b,hist_centers_b = exposure.histogram( image_b )\n",
    "\n",
    "fig,ax = plt.subplots( ncols=2,figsize=( 10,4 ) )\n",
    "\n",
    "ax[ 0 ].imshow( image_b,cmap='binary_r' )\n",
    "ax[ 0 ].set_title( 'Source Image, Blue Channel' )\n",
    "ax[ 1 ].plot( hist_centers_b,hist_b,lw=3 )\n",
    "ax[ 1 ].set_ylim( ( 0,256**2 ) )\n",
    "ax[ 1 ].set_xlim( ( 0,255 ) )\n",
    "ax[ 1 ].set_title( 'Histogram' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What interesting features do you observe from those histograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll revisit color images in `lesson05` when we examine satellite image data.  For now, we'll explore feature identification in grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Image Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as a team, you will explore how automatic feature identification and manipulation works.\n",
    "\n",
    "Magnetic resonance imaging (MRI) data are frequently stored as grayscale images where lightness corresponds to a response in the signal.\n",
    "\n",
    "<img src=\"http://jnm.snmjournals.org/content/46/1_suppl/151S/F2.large.jpg\" width=\"75%;\" alt=\"A series of MRI pictures after brain surgery.\"/>\n",
    "\n",
    "(For this portion, we are indebted to [Richard Barnes](http://rbarnes.org/)' outline on [Stack Overflow](https://stackoverflow.com/questions/49834264/mri-brain-tumor-image-processing-and-segmentation-skull-removing).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements that are commonly present, such as bones, are frequently removed in order to clean up the picture.  With a little bit of finesse, this process can be automated, since the skull generally occurs in the same place in all of the pictures.  This is called \"skull stripping,\" and we'll start by identifying and removing the skull from our set of images.\n",
    "\n",
    "We will use a set of MRI images obtained from the [XNAT](https://central.xnat.org/) repository, the `IGT_GLIOMA` data set provided by Ferenc Jolesz.  Each of these images contains a brain scan for a [glioma](https://en.wikipedia.org/wiki/Glioma), a relatively common kind of brain tumor.\n",
    "\n",
    "<img src=\"https://prod-images.static.radiopaedia.org/images/16514952/bff0d7dde0729713759de61d0d0a8f_big_gallery.jpeg\" width=\"33%;\" alt=\"Left frontal low-grade glioma.\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/scan.gif)\n",
    "\n",
    "Set up a display function for convenience in working with the image data.  Sometimes MRI data are stored in blue-green-red order instead of red-green-blue order, so we have a convenience function to deal with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowImage( title,img,ctype ):\n",
    "    plt.figure( figsize=( 10, 10 ) )\n",
    "    if ctype == 'bgr':  # switch to RGB\n",
    "        b = img[ :,:,0 ].copy()\n",
    "        g = img[ :,:,1 ].copy()\n",
    "        r = img[ :,:,2 ].copy()\n",
    "        rgb_img = np.stack( [ r,g,b ],axis=2 )\n",
    "        plt.imshow( rgb_img )\n",
    "    elif ctype == 'gray':\n",
    "        plt.imshow( img,cmap='gray' )\n",
    "    elif ctype == 'rgb':\n",
    "        plt.imshow( img )\n",
    "    else:\n",
    "        raise Exception( \"Unknown colour type\" )\n",
    "    plt.axis( 'off' )\n",
    "    plt.title( title )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load an image to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_path = './data/000079.png'\n",
    "image_brain = skimage.io.imread( brain_path )\n",
    "image_brain = image_brain >> 8\n",
    "ShowImage( 'Brain with Skull',image_brain,'gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine the histogram of the image data in grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_brain,hist_centers_brain = exposure.histogram( image_brain )\n",
    "\n",
    "fig,ax = plt.subplots( ncols=2,figsize=( 10,4 ) )\n",
    "\n",
    "ax[ 0 ].imshow( image_brain,cmap='binary_r' )\n",
    "ax[ 0 ].set_title( 'Source Image' )\n",
    "ax[ 1 ].plot( hist_centers_brain,hist_brain )\n",
    "ax[ 1 ].set_title( 'Histogram' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, this distribution tells us something about the kinds of methods that can be used to identify features.  For instance, notice that the skull and brain are very distinct in color from the background, verified by the histogram.  We can use this to select the skull and brain separately from the background.  One technique for accomplishing this is Li's method, which splits the histogram between peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = skimage.filters.threshold_li( image_brain )\n",
    "thresh = image_brain > threshold\n",
    "ShowImage( 'Thresholded Image',thresh,'gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`thresh` represents the original image classified into light and dark portions according to the histogram.  We use `thresh` as a \"mask\" to distinguish parts of the image.  Overlaying the mask and the original image, we can see the area of detection.  You should analyze and understand each line in the subsequent code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormask = np.zeros( ( image_brain.shape[ 0 ],image_brain.shape[ 1 ],3),dtype=np.uint8 )\n",
    "colormask[ thresh!=0 ] = np.array( ( 0,256,128 ) )\n",
    "blended = ( np.dstack( ( image_brain, )*3 ) + colormask * 0.8 ) / ( 255 * 1.8 )\n",
    "ShowImage( 'Blended Thresholded Image',blended,'bgr' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _scikit-image_ library, along with most image-processing libraries, is capable of identifying the separate components of a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.morphology\n",
    "markers = skimage.morphology.label( thresh )\n",
    "ShowImage( 'Connected Components',markers,'rgb' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know there should be two components, not one huge one, so we have to find a way to separate the brain from the skull.  One way is to play with the threshold used in Li's method.  Another is to \"erode\" the image, or expand the holes in the mask until they meet and cut off one area from another.\n",
    "\n",
    "Here, we attempt the latter.  We make a \"disk\" and apply it at the edges of the mask like a stamp.  This slightly expands the mask and makes the areas more distinct from each other.  We don't want to overdo it though.  (Try changing the disk size to see what we mean.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = skimage.morphology.disk(1)\n",
    "eroded = skimage.morphology.erosion( thresh,selem )\n",
    "ShowImage( 'Eroded Image',eroded,'gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of little connections that we need to \"snip\" to fully partition the contiguous areas.  We'll do this using lines and pixel coordinates, just like in Microsoft Paint.\n",
    "\n",
    "**This step, if necessary, is difficult to automate reliably.**  You may just have to erode more aggressively to avoid reliance on this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.draw\n",
    "rows,cols = skimage.draw.line( 110,53,120,53 )\n",
    "drawn = eroded.copy()\n",
    "drawn[ rows,cols ] = 0\n",
    "rows,cols = skimage.draw.line( 56,157,56,162 )\n",
    "drawn[ rows,cols ] = 0\n",
    "rows,cols = skimage.draw.line( 66,65,60,75 )\n",
    "drawn[ rows,cols ] = 0\n",
    "ShowImage( 'Drawn Image',drawn,'gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can visually see two connected components, corresponding to the skull and the brain, and the computer agrees with this.  The larger of these is the brain, so we can find the larger component, select only it from the mask, and thus strip the skull from the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-segment the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = skimage.morphology.label( drawn,connectivity=1 )\n",
    "ShowImage( 'Connected Components',markers,'rgb' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the area taken by each component.  (Ignore label 0 since this is the background.)\n",
    "marker_area = [ np.sum( markers==m ) for m in range( np.max( markers ) ) if m!=0 ] \n",
    "\n",
    "# Get label of largest component by area\n",
    "largest_component = np.argmax( marker_area ) + 1  # add 1 since we dropped zero above                        \n",
    "\n",
    "# Get pixels which correspond to the brain.\n",
    "brain_mask = ( markers==largest_component )\n",
    "\n",
    "# Make a copy of the original image and select only those pixels corresponding to the brain.\n",
    "image_brain_out = np.dstack( ( image_brain, )*3 )\n",
    "image_brain_out[ brain_mask==False ] = ( 0,0,0 )\n",
    "ShowImage( 'Extracted Brain',image_brain_out,'rgb' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's okay, isn't it?  It's missing a few pieces due to the erosion, but we can relax the image back out a bit by eroding the mask instead of the image.  We won't bother to do that right now, but leave it as an exercise for you to attempt if you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can build a pipeline to handle _all_ of the images in the data set at the same time.  We have two options:  we can load all of the images at once and process them, or load them one at a time and process them one at a time.  The former requires more memory, so it may not be preferred for extremely large data sets, but we'll use it here since there are some tools to handle that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_files_path = './data/0000*.png'\n",
    "brain_images = skimage.io.imread_collection( brain_files_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we rebuild our immediately preceding pipeline as a function, we can loop over and output the stripped images.\n",
    "\n",
    "Compose a function `strip_skull` which accepts an image (as a NumPy array) as argument and returns the stripped image as a NumPy array.  (It shouldn't worry about the `drawn` portion since that can't be easily generalized.  Just erode more aggressively.)  Verify that it works for one of the images besides the one we just tested.  Then apply it to the whole pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_skull( image ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ( brain_image,brain_image_path ) in zip( brain_images,brain_images.files ):\n",
    "    brain_stripped = strip_skull( brain_image )\n",
    "    ShowImage( brain_stripped )\n",
    "    stripped_image_path = brain_image_path.replace( '.png','-stripped.png' )\n",
    "    skimage.io.imsave( stripped_image_path,brain_stripped )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
